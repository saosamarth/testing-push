{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NLIZb5Ha7ly1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "from collections import Counter, defaultdict\n",
        "import re\n"
      ],
      "metadata": {
        "id": "tbe6QZFV708W"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Tokenization Function"
      ],
      "metadata": {
        "id": "QqJjU7-P957q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Tokenizer\n",
        "def tokenize(text):\n",
        "    # Split text into words using regular expressions\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "DnUFBK4l72Uj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Preprocess Corpus"
      ],
      "metadata": {
        "id": "SgWkdz7k99CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Corpus\n",
        "corpus = \"\"\"\n",
        "Natural language processing enables machines to understand human language.\n",
        "Text generation is a key application of NLP. Language models predict text sequences effectively.\n",
        "\"\"\"\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_corpus(corpus, vocab_size=5000):\n",
        "    tokens = tokenize(corpus)  # Tokenize text\n",
        "    freq_dist = Counter(tokens)  # Count token frequencies\n",
        "    vocab = set([word for word, _ in freq_dist.most_common(vocab_size)])  # Limit vocabulary size\n",
        "    processed_tokens = [word if word in vocab else '<UNK>' for word in tokens]  # Replace rare words with <UNK>\n",
        "    return processed_tokens, vocab\n",
        "\n",
        "tokens, vocab = preprocess_corpus(corpus)\n"
      ],
      "metadata": {
        "id": "KQIwKtHP732k"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data into Train, Validation, and Test Sets"
      ],
      "metadata": {
        "id": "tw35x6wE-AgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training (70%), validation (10%), and testing (20%) sets\n",
        "def split_data(tokens):\n",
        "    train_size = int(0.7 * len(tokens))\n",
        "    val_size = int(0.1 * len(tokens))\n",
        "    train_data = tokens[:train_size]\n",
        "    val_data = tokens[train_size:train_size + val_size]\n",
        "    test_data = tokens[train_size + val_size:]\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "train_data, val_data, test_data = split_data(tokens)\n"
      ],
      "metadata": {
        "id": "2954qU_Y75v-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Backoff Model (LM1)"
      ],
      "metadata": {
        "id": "rI0QOwZK-EGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BackoffModel:\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.ngram_counts = defaultdict(Counter)\n",
        "\n",
        "    def train(self, data):\n",
        "        for n in range(1, self.n + 1):  # Train for 1-grams to n-grams\n",
        "            for ngram in zip(*[data[i:] for i in range(n)]):\n",
        "                self.ngram_counts[n][ngram] += 1\n",
        "\n",
        "    def get_probability(self, context, word):\n",
        "        for n in range(len(context), 0, -1):  # Back off from higher-order to lower-order n-grams\n",
        "            ngram = tuple(context[-n:] + [word])\n",
        "            if ngram in self.ngram_counts[n]:\n",
        "                return self.ngram_counts[n][ngram] / sum(self.ngram_counts[n][tuple(context[-n:])].values())\n",
        "        return 1e-6  # Small default probability for unseen words\n"
      ],
      "metadata": {
        "id": "A5PsUbOh77Q2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Interpolation Model (LM2)"
      ],
      "metadata": {
        "id": "_Oowr8nM-I2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aUDMaqWB-HWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Backoff Model\n",
        "lm1 = BackoffModel(n=4)\n",
        "lm1.train(train_data)\n",
        "\n",
        "# Train Interpolation Model\n",
        "lambdas = [0.1, 0.2, 0.3, 0.4]\n",
        "k = 0.1\n",
        "lm2 = InterpolationModel(n=4, lambdas=lambdas, k=k)\n",
        "lm2.train(train_data)\n"
      ],
      "metadata": {
        "id": "s5dRfK8x7_ag"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Models with Perplexity"
      ],
      "metadata": {
        "id": "w4ZpOvfc-hrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InterpolationModel:\n",
        "    def __init__(self, n, lambdas, k):\n",
        "        self.n = n\n",
        "        self.lambdas = lambdas\n",
        "        self.k = k\n",
        "        self.ngram_counts = defaultdict(Counter)\n",
        "\n",
        "    def train(self, data):\n",
        "        for n in range(1, self.n + 1):  # Train for 1-grams to n-grams\n",
        "            for ngram in zip(*[data[i:] for i in range(n)]):\n",
        "                self.ngram_counts[n][ngram] += 1\n",
        "\n",
        "    def get_probability(self, context, word):\n",
        "        probabilities = []\n",
        "        for n in range(1, self.n + 1):\n",
        "            # Construct n-gram\n",
        "            ngram = tuple(context[-(n-1):] + [word])\n",
        "\n",
        "            # Count of the n-gram\n",
        "            count = self.ngram_counts[n][ngram] + self.k\n",
        "\n",
        "            # Count of the context\n",
        "            context_ngram = tuple(context[-(n-1):])  # Context is n-1 words\n",
        "            context_count = sum(self.ngram_counts[n][context_ngram].values()) if context_ngram in self.ngram_counts[n] else 0\n",
        "\n",
        "            # Total count with smoothing\n",
        "            total = context_count + self.k * len(vocab)\n",
        "\n",
        "            # Probability calculation\n",
        "            probabilities.append(count / total if total > 0 else 0)\n",
        "\n",
        "        # Interpolated probability\n",
        "        return sum(l * p for l, p in zip(self.lambdas, probabilities))\n"
      ],
      "metadata": {
        "id": "KDXlreMD86Bj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vI8JcjLL-P5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train InterpolationModel\n",
        "lambdas = [0.1, 0.2, 0.3, 0.4]  # Example weights\n",
        "k = 0.1  # Add-k smoothing factor\n",
        "lm2 = InterpolationModel(n=4, lambdas=lambdas, k=k)\n",
        "lm2.train(train_data)\n"
      ],
      "metadata": {
        "id": "lvdsp4Q09Nd5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate perplexity for both models\n",
        "perplexity_lm1 = calculate_perplexity(lm1, test_data)  # Backoff model\n",
        "perplexity_lm2 = calculate_perplexity(lm2, test_data)  # Interpolation model\n",
        "\n",
        "# Print results\n",
        "print(f\"Perplexity of Backoff Model (LM1): {perplexity_lm1}\")\n",
        "print(f\"Perplexity of Interpolation Model (LM2): {perplexity_lm2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh2oAjyy9QAK",
        "outputId": "6715cd7f-d40b-4aee-8138-a8c855545ff2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Backoff Model (LM1): 999999.9999999999\n",
            "Perplexity of Interpolation Model (LM2): 20.000000000000004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Text"
      ],
      "metadata": {
        "id": "svON873F-Y9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed, max_length=50):\n",
        "    text = seed[:]\n",
        "    for _ in range(max_length - len(seed)):\n",
        "        context = text[-3:]\n",
        "        next_word = max(vocab, key=lambda w: model.get_probability(context, w))\n",
        "        text.append(next_word)\n",
        "        if next_word == '<END>':\n",
        "            break\n",
        "    return ' '.join(text)\n",
        "\n",
        "# Example Text Generation\n",
        "seed = [\"natural\", \"language\", \"processing\"]\n",
        "generated_text_lm1 = generate_text(lm1, seed)\n",
        "generated_text_lm2 = generate_text(lm2, seed)\n",
        "\n",
        "print(\"Generated Text (Backoff Model):\", generated_text_lm1)\n",
        "print(\"Generated Text (Interpolation Model):\", generated_text_lm2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSrkzy0V9Rlf",
        "outputId": "c393abb0-3690-448e-e8e2-ae41d52d76d4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text (Backoff Model): natural language processing language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language\n",
            "Generated Text (Interpolation Model): natural language processing enables machines to understand human language text generation is a key application of language text generation is a key application of language text generation is a key application of language text generation is a key application of language text generation is a key application of language text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0VQANRdn9YMy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}